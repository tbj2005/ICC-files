### TOPOOPT: Co-optimizing Network Topology and Parallelization Strategy for Distributed Training Jobs

TOPOOPT 是我们提出的一种全新的用于深度神经网络训练负载的直连网络，它通过三个维度（计算、通信、网络拓扑）联合优化了分布式训练过程。

目前训练负载不能满足在胖树互联的设计中产生的数据中心流量：1. 大型 DNN 训练的通信负载是随着 worker 数目的增加动态增长的；2.  DNN 训练的通信模式是由并行策略决定的。TOPOOPT 使用可重构光交换机和专用小组为每个训练业务创造专用的分区，并在每个分区联合优化了拓扑和并行策略。为了达成这一目标，我们尽力解决了寻找最佳拓扑的算法挑战，例如怎样流量计算、通信和拓扑维度的巨大搜索空间，还有多种操作挑战，例如光交换技术怎样和 DNN 模型的通信模式匹配。

我们将拓扑和并行策略共同优化问题描述出一个离线选择优化架构。我们的优化策略在优化并行策略和优化网络拓扑间选取。先在假设固定拓扑的情况下，搜索并行策略空间，然后将通信需求输入寻拓扑算法中。更新后的拓扑然后输回并行策略搜索算法。这个选择过程会重复，直到系统收敛到一个已经优化好的并行策略和拓扑。

寻找和优化 DNN 网络拓扑是困难的，因为理想的网络拓扑需要同时满足两种条件：1. 能高效完成大规模 AllReduce 传输；2. 确保在模型并行传输中有较小的跳数。为了满足这些目标，我们提出了一种新的机遇群原理的技术，叫做 TotientPerms，可以利用 AllReduce 通信的可变性。我们的 TotientPerms 方案建立了一系列 AllReduce 排列，它们不仅可以高效地传输 AllReduce 的流量，还能很好的安放以保证模型并行流量的传输。

DLRM 流量模式：DLRM 是一种推荐模型，一般有千亿级参数，这是因为它们巨大的嵌入表。只是用数据并行来做分布式会导致大规模的 AllReduce 传输。

大多数业务的 worker 使用在 32 到700 间（Meta 集群），符合近期工业主流宣布的数值，每个 worker 上只有单个 GPU 。


