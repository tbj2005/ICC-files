# 面向混合并行场景下AI流量的光互连数据中心网络调度

## 背景：

在大模型训练过程中，由于数据集合参数规模越来越大，单卡早已无法支持当前业务的训练，因此需要使用并行方案将单卡的训练压力稀释到多卡。AI 大模型训练任务~~~~需要多卡多节点分布式并行完成，集群规模扩展到万卡及以上数量级。当前 GPU 算力快速增长，网络通信正在拖大模型训练的后腿。

为了加速网络通信，可以引入光互连数据中心，利用其吞吐量高的优势，为通信提供更高的带宽，以缩短通信时间。然而，当前光互连 DCN 中，POD间的光连接拓扑重构延时高与大模型高效的传输需求并不符合，因此当前的大模型训练没有大规模采用光互连数据中心。在当前，计算和通信具有依赖性，在计算的时候链路往往是闲置的，计算资源和带宽资源总会有一个偷懒，不完全符合当优质帕鲁的要求。但是，由于AI流量具有时序性，可预测性强，可以通过设计面向AI流量的调度算法，将重构延时隐藏在计算过程中，扬长避短，提高训练过程中的网络通信能力。

混合并行是当前大模型训练的热门方案，具体是对同一训练模型采用多种并行方案，即复用分布式数据并行和模型并行，融合两种并行策略的优势以优化训练过程，降低单卡负担。其中，模型并行有多种方案，如流水线并行、张量并行等。

分布式数据并行将数据集均分成数据并行度大小数目的子集，从一轮迭代开始，在每个节点使用不同的子集训练相同大小的模型，然后每个节点再将得到的参数进行聚合和下发，然后开始下一轮的迭代，可以大幅降低节点训练数据量大小。节点间的通信可以使用参数服务器完成，但是参数服务器和其他 NPU 工作时间是错开的，没有高效利用 NPU 的资源，且在通信过程中参数服务器带宽压力过大也会导致瓶颈。在之前的工作中引入了在网计算这一项技术，缓解了参数服务器带宽压力过大的问题，但是网络中可能并不能满足所有参数服务器全部被取缔，因此仍会存在参数服务器，并不能完全解决上面的问题。而另一种通信算法 Ring-Allreduce可以解决上面的问题，通过回环同步不同 NPU 之间的梯度。

流水线并行是将训练过程分成若干个层，每一层可以分开训练上一层的输出可以作为下一层的输入，这样可以大幅降低节点内模型大小，防止模型爆掉节点的内存。然而，这样会导致 NPU 利用率极低，因此微软提出了 Gpipe 方案，即将数据集也分成若干子集，每个子集独立通过流水线完成训练过程，此时网络中 NPU 利用率大幅提高，且随着子集粒度的降低，网络中 NPU 利用率也会随之降低。

![朴素流水线并行和Gpipe](https://img-blog.csdnimg.cn/img_convert/15c54d10e976d35c96e57965145300e8.png)

张量并行是指将训练过程中常用的矩阵乘法进行行并行或是列并行，分别在不同节点计算后通过加和或是拼接的并行方案。这类并行方案可以切分很大的模型，但是通讯量大，通信一般只在节点内部做多卡通信，不会做跨节点通信。

总结：

- **数据并行**：将数据集分割成多个子集，在多个设备上分别处理这些子集。这种方式能显著提高数据处理速度，但需要确保子集之间处理结果一致。
- **流水线并行**：将模型按层分割成若干块，每块都交给一个设备进行处理。这种方式能提高设备的利用率，但需要确保流水线中各阶段正确传递中间结果。
- **张量并行**：将模型的不同部分分配给不同的设备进行处理。这种方式能充分利用各种设备的计算能力，但需要注意设备之间的通信开销。

三种并行方案的通信量为：张量并行 > 数据并行 > 流水线并行

![image-20240619210947461](C:\Users\tbj20\AppData\Roaming\Typora\typora-user-images\image-20240619210947461.png)

上图是一个三维混合并行方案。数据集首先被分成两个子集，分别使用上下两层 NPU 训练；在每层 NPU 中使用流水线并行分别进行训练，这里模型被分成 4 层，每层使用一列 NPU 处理；在每一层流水线中，一列 4 个 NPU 使用张量并行训练这一层模型。可以看出，混合模型融合了不同并行方案的优势，每个 NPU 需要训练的数据集大小和模型大小被大幅降低了。在训练大模型时使用混合并行方案，不同 NPU 间的通信情况不同，这也对合理的网络调度提出了要求。

## 问题提出：

在混合并行场景下，使用光网络加速AI大模型训练任务，利用AI流量通信需求具有时序性，可预测性强的特点，规划 NPU 使用、流量调度和光互连拓扑的重构。

## 优化思路：

具体来说，若数据并行度为 N，训练集大小为 D，光网络中 Pod 数目为 P。将这些 Pod分成 N 份作数据并行，每一份训练 mini-batch 数据集大小为 $\dfrac{D}{N}$，并训练完整的模型参数。在每一组数据并行的 Pod 中将 NPU 再分成 M 组以进行层数为 M 的流水线并行。将 mini-batch 再细分成 micro-batch，使用 Gpipe 方案进行以提高 NPU 利用率。流水线的每一层都由一个 server 完成，一个 server 内有 T 张 NPU，可以进行并行度为 T 的张量并行。此时为了训练这个模型，我们使用了 $N*M*T$ 张 NPU。

训练过程：

1. 将数据集分成 N 个 mini-batch，分给不同的数据并行单元；
2. 将每个 mini-batch 再分成 M 个 micro-batch，并进行数据流水线并行；
3. 在数据进入每层流水线后，server 内的 NPU 间使用张量并行。而张量并行的通信和计算都是在 server 内进行，server 内 NPU 间有着高容量的卡间带宽，可以满足张量并行的高通信量要求，具体张量并行的过程与调度无关；
4. 每次张量并行求得每一层流水线的结果后，需要在各层流水线间进行通信以进行前后向传输，在完成反向传播后，梯度汇总并进行参数更新，这个传输过程可能是跨 Pod 的，因为一个数据并行单元可能包含多个 Pod，因此需要使用光交换资源和 Pod 内带宽资源。
5. 在完成后向传输后，完成一次流水线操作，获得了 mini-batch 训练更新的参数，还需要通过 ring-Allreduce 更新数据并行单元的参数，此时也需要使用光交换资源

可以看到，整个过程使用了两次光网络资源，需要进行拓扑重配以最大化提供的带宽。

可以发现，在流水线并行完成反向传播后并不是马上去做数据并行的的通信，而是要等待梯度汇总和参数更新，在这段时间里，网络是闲着的，就可以把这段时间用来将拓扑从流水线转成数据并行；在模型训练过程中，第一个 micro-batch 的训练时，网络也是空着的，可以将拓扑再从数据并行转到流水线，以进行后续的流水线操作。

此时，训练过程中网络空闲的时间就被用于拓扑重构，隐藏了拓扑重构的时间，最大化地利用了光网络拓扑变化灵活，吞吐量大的优势。

对于网络而言，我们希望的是在未知混合并行方式和并行度，未知训练数据量和各卡训练通信时间，且无需改变现有训练方案的情况下，同时，网络也无法截取流量中的包，只通过网络中可获得的数据，来预测网络中的流量和连接拓扑。

由于 AI 流量的时序性，每隔一段时间，流量需求便会循环一次，直到完成训练，可以依据这一特点，来预测同步训练下每一次所有卡完成训练后的流量需求，并在训练过程中改变光连接，从而使得在 NPU 训练过程中，光网络并不是赋闲的，而是在利用这段时间改变拓扑重构，在利用光网络高吞吐量和灵活性的同时，规避重构时间长的风险。问题不在这个基本思路里，这种重叠时间的思路在很多地方已经用到了，不具有新颖性。

主要问题在于预测。因为网络是不知道整个模型什么时候完成了一次混合迭代，因此不好预测混合并行的周期数，而且至少网络在第一个迭代是完全不知道循环是怎样的，因此会产极高的延迟。因此我们需要一个合理的方案快速获得模型一次迭代的拓扑队列，且由于大模型的规模，模型收敛需要的迭代次数极高，因此后续完全匹配的拓扑队列对模型训练的增益将完全覆盖前几次迭代的损失。

但是，网络的效率仍然没有被完全发掘，因为我们只是在训练过程中使得网络没有停止工作，但是通信过程中，NPU 仍然在摸鱼。。。要解决这个问题，应该在训练方面下手，与光通信的调度无关。

说回算法，如前所述，我们要预测的其实是循环的一个周期，即最小周期预测。方案如下：

1. 初始化：拓扑队列为空，循环次数为0；
2. 在训练过程中执行如下操作：

​        a. 若循环次数为0，拓扑队列为空，说明这是第一次循环的第一次通信，在训练过程置空所有光连接拓扑，在获得通信需求后计算拓扑连接方案，并将该拓扑放入拓扑队列，并将循环次数置0；

​        b. 若循环次数为0，拓扑队列不为空，说明这是第一次循环的非首次通信，不修改上次的拓扑，在获得通信需求后计算拓扑连接方案，判断连接方案是否与队列首个元素相同。如果相同，判定为出现循环，标记循环次数为1。反之，如果不相同，判定为在循环内，将该拓扑放进拓扑队列，并将循环次数置0；

​        c. 若循环次数大于0，说明已经获得了拓扑队列，此时取拓扑队列首个元素，按此拓扑在训练过程修改光连接。获得通信需求，计算拓扑连接方案，并与取得的方案做匹配。若相同，则直接开始进行光通信，若遍历完一次拓扑队列，将循环次数加1，并进入下一次循环；若不相同，说明此时获得的周期并不是最小预测周期，而是这个周期中的一部分，此时，需要重构拓扑队列，并将循环次数置0。

​        重构队列的具体方案为：将原拓扑队列复制循环次数次，再加上此次循环已经遍历的部分，就是新的拓扑队列。

​        d. 训练收敛时，结束算法

LLM 的流量特征：

LLM 的训练会在每台主机上产生少量周期性爆发流量，如400 Gbps 网络容量会被瞬间满足，持续时间几秒到几十秒不等，梯度同步导致了此类流量的产生。这会导致等价多路径哈希极化（多层哈希后流量不均），造成不确定流量分布的问题。且训练产生的通信连接需求往往只有数十到数百条，远远低于云计算。

LLM 的训练会要求GPU同步迭代，此时会对单点故障更加敏感。

GPU 规模：每个主机 8 个 GPU，每个数据中心 1875 个主机，共 15000 个 GPU，网卡吞吐量约 400 Gbps

传统数据中心和 LLM 匹配度不高。这是因为传统数据中心使用 ECMP 方式作为负载平衡方案。ECMP 假定当网络中存在大量流量时，哈希算法可以有效地将所有流量平均分配到所有等效路径上。这一假设在云计算流量模式下成立，因为云计算流量模式一般会产生数百万流量，不易出现极化。但是 LLM 流量数目低，数据量大，即大象流，容易产生哈希极化，严重影响效率。甚至，经典三层 clos 架构（ToR、Aggregation、Core）会使用三次哈希，因为每个流量哈希的五元组保持相同，这种级联哈希的效果会导致更严重的哈希极化。

当前的只是将预测和重叠时间两个比较老套的方案结合在了一个新的场景而已。

算法可已知模型并行方案和并行度，数据量等信息，可加入动态场景的调度，此时循环会发生变化，可以尝试为多个LLM业务做调度。其次，还可以联合调度混合并行方案和集合通信方案，并与INC结合。比如一个数据中心可以动态地为多个LLM提供服务，此外，联合调度混合并行方案和集合通信方案和INC使用方案。

## 拓扑选择：直接连接 or 间接连接？

直接通信：速度快，延迟低，扩展性差

间接通信：速度相对慢，延迟高，扩展性和灵活性高

混合通信：服务器内搭载多张 GPU，GPU 间采用 NVL、PCLe、QPI 高速互联，由于 RDMA 等技术的使用，服务器内 GPU 间通信带宽高，延时低。但是服务器内 GPU 的通信复杂度阻碍了服务器内 GPU 数目的增长，此时在服务器间采用间接连接以提高网络的可扩展性，服务器间连接可以采用 clos 结构组成三层 Fat-tree （？）。服务器间的通信引入光网络，利用其带宽高，可以适应倾斜流量的特性，来加速训练过程中倾斜的集合通信流量。

## 混合并行方案的选择：

DP+PP+TP

模型并行是三种并行方式开销最大的，因此将其放在服务器中，使用服务器内的高速互联来处理。在模型并行中会产生 AllReduce 流量，这部分流量会通过 NVL 快速传输，无需调度，模型并行度由服务器内 GPU 数目等因素决定。

数据并行通信开销高于流水线并行，尽量使用光网络，发挥其倾斜性和高带宽的优势，因此第一层并行采用数据并行，具体的集合通信方案的选择按下不表。

流水线并行通信开销最低，只在有必要时使用光交换。

若已知训练数据量、参数量和并行度，可以估算各处训练时间和集合通信时间，可以以此作为调度的前置信息。

## 集合通信算法的选取：

张量并行要进行数据同步，在服务器内采用 AllReduce 操作；流水线并行只有 send-recv 流量，传输的是上一层流水线的输出数据，无需集合通信算法；数据并行传输的是每个数据并行组的参数，其中的集合通信算法待选取，最经典的两种算法如下：

parameter server：参数服务器存在带宽瓶颈，在参数服务器和其他训练节点工作时间不重叠。但是可以通过部署 INC 来解决这一问题，此时网络设备就可以作为参数服务器，不会出现上面的情况。

ring-AllReduce：同样可以解决传统 parameter server 架构的问题，但是由于没有聚合操作，无法使用 INC，且随着 ring 的增大，集合通信效率会大大降低。

上面所说的数据并行->流水线并行->张量并行方案是针对数据量而言的，但是当考虑了集合通信算法后，混合并行方案也许会有不同。

若采用 parameter server 算法，数据并行组需要尽量处于同一个 pod 下，从而避免跨机架聚合，且能大幅降低 oxc 中的流量，因此混合并行方案可以为：流水线并行->数据并行->张量并行。（不一定就要用这种并行方式，因为此时 ToR 上的流量就是整个模型的数据量，网络瓶颈就在 ToR 上行带宽，会拉低通信效率，所以这部分还需要权衡）；

若采用 ring-AllReduce 算法，此时因为数据并行通信数据量更大，混合并行量可以为：数据并行->流水线并行->张量并行

## 多 LLM 业务在 server 使用、集合通信方案和集合通信上的共同调度：

数据中心网络中可能存在多个 LLM 业务，其出现也可能是动态的。怎样选取这些业务的并行方案和集合通信方案，怎样调度这些业务的集合通信流量？

以通信方案为例，在 INC 资源足够时，采用 parameter server 架构显然更好，但是如果 INC 资源被其他业务使用了，采用 AllReduce 可能就是更好的方案。

集合通信可以通过之前估算的训练和通信时间调度，通过预测流量周期以在训练过程完成光网络重构。

### 集合通信算法：

#### Ps-worker:可用于数据并行

确定拓扑方便（只需确定数据并行单元部署位置即可），使用限制多（单 pod、ToR 上的 INC 资源），完全无需使用 OXC。

#### AllReduce 类：可用于数据并行

难以确定拓扑，不仅需要确定每个数据并行单元的位置，还需要确定这些数据并行单元怎样互联成环/树。

##### 环算法：ring-AllReduce

单节点接受发送数据量达到最小，输入输出带宽用满，但是如果不只考虑带宽，再考虑每次传输的延迟

使用线性通信开销模型,$T = \alpha+S/B$

$T = （N - 1）(2\alpha + 2\dfrac{S}{NB}+\dfrac{SC}{N})$

其中， $\alpha$为设备间的通信固有延时，S为数据块大小，N 为节点数目，B为节点带宽，C为每字节计算耗时。

当 S 极大时，固有延时 $\alpha$ 可以忽略，此时随着N 的增大，延时 T 与 N 无关，为$T = 2\dfrac{S}{B}+SC$。

但是当 S 特别小，或者 N 特别大时，后两项就被忽略了，此时 $T = 2(N-1)\alpha$，此时延时由固有延时决定，随着 N 的增大持续增加。这就是环算法的缺陷，正因如此，树算法才被开发出来。

##### 树算法：tree-AllReduce

树算法实际在英伟达 NCCL 投入使用的方案是双二叉树法，使用两个对称二叉树解决传统单二叉树算法节点输入输出带宽量不同的问题。

根据论文，其延迟为$T \leq 4\alpha\log N+\dfrac{2S}{B}+2\sqrt{8\log N\alpha\dfrac{S}{B}}+(\sqrt{2\log N\alpha S}+S)C$

可以看出树算法对 N 极大的情况表现比环算法好得多。

但是树算法在同样的节点数目情况下，需要建立的连接数比环算法多得多，拓扑的复杂度更高。

#### 分层 ring-AllReduce

即将环分为两层，可以缓解 N 极大带来的延迟提高。连接数多于普通环算法，但是总时间在 N 较大时更低。但是如果 N 非常大，树算法的表现更好。

#### AlltoAll: 只用于 MoE 并行

1. 为已知混合并行需求的业务提供服务；

2. 假设分别使用不同的集合通信算法，分别寻找合理的 GPU 部署方案和拓扑，选择最优的部署方案和通信算法

部署方案的优异程度怎样确定？

1. MLU

2. 集合通信是否需要重构 OXC，重构次数多少？

3. 在多业务情况下，不同时间集合通信复杂度不同，可能在某时刻只有一个业务需要通信，可能某时刻有多个业务同时有集合通信需求，如何联合考虑所有情况下通信时间的优劣？

背景：传统的数据中心网络在训练大模型时，会由于哈希极化现象，导致网络中的流量不均衡，从而造成网络瓶颈。而光互连由于可以通过灵活变更拓扑，使得网络可以适应倾斜流量，在理论上，表现会远远优于传统的数据中心网络。但是由于技术原因， oxc 的端口数目在大规模场景下难以支撑其实现全部 pod 间的互联，为了缓解这种情况，我们引入了 INC 技术来降低网络中的流量大小，使得有限的端口数目能够应对更多的流量场景，同时还可以通过联合优化并行策略和通信算法，进一步缓解这一问题。

1. 并行策略级：混合并行的策略已经有研究了，在 SmartMoE、Alpa等文章中已经讨论过多种混合并行方案怎样选择，和专家并行时 expert 如何部署的问题。在这一部分可以尝试再将 INC 引入考虑场景，进一步优化在有 INC 场景下的选择；

2. 通信算法级： PP 流量比较简单，无需特定的通信算法，TP 常常被放在服务器内部使用 NVLink 高效互联，不会占用网络的带宽，因此不在我们考虑范围内。问题主要集中在 EP 和 DP 两种并行方案上，其中 EP 每次迭代都会在 expert 间出现两次不均匀的 AllToAll 通信，这部分通信在 DeepspeedMoE 中已经做了优化，目前针对 EP 中的集合通信优化除此之外几乎没有其他的方案；DP 需要做 AllReduce ，存在多种已成熟的方案可供选择，当下主流的算法为 ring-AllReduce，但在英伟达的集合通信库中，使用的是 tree-AllReduce，在特定的场景中，也有不同的 AllReduce 算法被开发，如果将 INC 及其限制引入考虑场景，那么参数服务器也可以是一种待选择甚至更加优秀的方案，而网络中大概率不是只跑一个业务，对不同的业务来说，可能更适合采用不同的算法，算法的选取不仅和业务本身有关，还与并行策略中怎样部署相关的并行单元有关；

3. 流量调度级：确定 OXC 的连接方案，使得在网络拓扑不发生变动的情况下，逐个处理业务通信需求，从而使得网络的训练效率更高。如果网络拓扑不能满足每一个业务的通信需求，则在每一次迭代的过程中，网络都需要重构，而每次迭代网络重构延时数百毫秒，在动辄千万次迭代的大模型场景里是不可容忍的。这一部分如果要考虑的很细的话非常复杂，虽然网络中同一个业务的流量是周期性的，但是多个业务放到一起，情况就会变得非常复杂且不易调度，难以使用周期性质，因此这一部分目前看先按照最简单的方案，就是通信需求分开，不同的业务不能同时使用网络资源。

#### 

#### 问题描述：

在在线模式下，数据中心网络会随时出现分布式的混合并行训练业务，每个训练业务存在若干次迭代。已知每个业务各并行方式的并行度，怎样将业务的各并行单元分配给网络中各 pod 上的 GPU 才更合理？在确定业务部署方案后，每个业务在每次迭代的通信过程应当采用怎样的聚合策略和路由策略？每个业务分配的带宽量又是多少？

#### 问题输入：

1. 网络特征及各项参数：三层 clos 脊叶网络，ToR 与 server pool 间的带宽，oxc 连接数目，单 OXC 连接带宽量，每个 ToR 下的 server pool 资源池，ToR 数目，单 ToR oxc 端口数目，单个 worker 上搭载的 GPU 数目；

2. 业务特征：每个业务的并行方案和各层并行度，业务每次迭代的训练用时，业务通信量和通信类型；

3. 交换机聚合器资源；

#### 假设：

1. oxc的传输方案选择全双工，也就是说一条连接可以同时为两个方向的流量提供相同大小的带宽；

2. 当前只考虑单跳转发，不考虑两跳及以上；

3. 由于张量并行一般在服务器内部进行，不会使用到外部带宽，因此混合并行中不考虑张量并行；

4. 不同并行方式下各并行单元间的通信模式：EP：AllToAll；PP：Send-Recv；DP：AllReduce，混合并行下三种流量都会存在；

5. EP 并行的 AllToAll 通信由于 Gate 概率问题不会是均匀的，为了简化考虑，认为该过程为单纯的数据块转置操作，此时每对并行单元间都会出现通信需求；

6. DP 并行的 AllReduce 通信可以使用在网聚合，但是并不是每次通信，在网聚合的资源都是空闲的，因此网络不仅要有在网聚合的能力，在没有在网聚合是也能通过相应的 Reduce 算法来解决。具体实现时在打包数据的时候把是否使用在网聚合放进 head 段用来识别聚合方案就可以了，配合路由方案选择的算法按理来说可以实现不同迭代使用不同聚合方案的操作;

7. 假设网络中所有 ToR 都可以使用在网聚合；

8. 在网聚合可以使用 ring 拓扑，这比使用 PS 结构更加合理。相对于 PS 结构来说，ring 拓扑不会在任何地方产生带宽瓶颈和聚合器资源瓶颈，也无需使用二级聚合。

9. 在网聚合需要使用聚合器资源，会使用 ToR 上的带宽资源，假设网络中的聚合器资源不能完全满足所有任务的通信需求，此时我们不得不使用 AllReduce 算法。按理来说，每个业务使用的聚合器资源应该和带宽分配匹配，防止因为计算速度过慢导致拖带宽的后腿；

10. PP 的流量是在训练过程产生的，使用 GPipe 操作后，每层间的前向传播时间不是错开的，而是通过 microbatch 进行了流水化，传输的是每层输出的结果；DP 的流量是各数据并行单元训练完毕后进行的，传输的是模型梯度；EP 的流量是 token ，在专家并行单元间会进行一来一回两次传输，第一次传输是训练完成后还回 token，第二次传输是训练开始前交换 token。

11. 在混合并行过程中，第一层为数据并行，即将数据集分成多份，并将复制同样数目的专家单元，将这些数据集输入各自的专家单元；然后每个专家单元由若干 moe 层组成，这些 moe 层间存在通信；在 moe 层中有若干 expert ，每个 GPU 上可能有多个 expert，这些 GPU 间存在 AllToAll 通信。其中，流水并行和专家并行都是和训练计算过程深度交叉，可以算作训练的一部分，且相对于数据并行，它们的通信量不高；

12. 当同一个 ToR 上存在多个 DP 单元时，ToR 和 server pool 之间三种并行的流量并存，DP 并行产生的 AllReduce 流量远大于其他两类流量，EP 和 PP 流量相对较小，且流量相对 DP 不好预测，EP 更是会出现流量不均的情况。此时由于流量大小的原因，其需求的带宽大小也是 DP 更大，此时假设只考虑 DP 流量，另外两种流量带宽分配不大，相对 DP 可以忽略不计。但是一旦出现一个 DP 单元被放在两个不同 ToR 下，此时不得不为其分配一条连接以保证训练时的传输需求，此时如果这条连接上存在 DP 流量，则考虑传输时间时只考虑 DP 流量传完的时间，反之，则这条连接上流量的传输时间不参与考虑。

#### 待解决的问题：

1. 部署问题：初始业务 GPU 部署问题，将 GPU 部署进网络后怎样部署各并行单元；

2. 流调度问题：为每个业务的通信计算两种 AllReduce 方案，分别是使用在网聚合和不使用在网聚合；在在线模式预测业务的通信需求，并为其确定通信方案，若使用在网聚合，确定使用的聚合器资源及资源位置，若不使用，则确定网络拓扑和带宽分配，确定是否使用 oxc 重构。在通信过程中，不同迭代下的通信方案可以不同，用以适应不同场景；

3. 拓扑重构什么时候做？怎么做？怎么算新拓扑？

#### 优化目标：

GPU 平均训练时长，由于未知迭代次数，无法计算平均完成时长，因此摒弃使用平均化时间的思路，转而提高 GPU 利用率。

##### 调度方案

避免使用 time-slot 的方法，而是在每个通信需求开始和结束时都做一次带宽分配优化。

定时检查更换拓扑，以适应不同通信需求，或是新业务加入和业务完成

##### 业务并行单元部署方案：（这里存在两种不同的部署方案，一种 DP 通信使用 oxc，一种 PP 通信使用 oxc；还有要增加复用拓扑的考虑）

###### 备选方案1：

1. TP 组使用在服务器内部的 GPU 间进行；

2. 若干 worker 构成 EP 组，并作为一个 EP 层，worker 间采用 AllToAll 操作，组内 worker 间通信量不均且不稳定，放在同一个 ToR 下；

3. 每个 EP 层间使用 Pipeline 并行，通信量稳定且流量不高，可以放在同一 ToR 下，也可以不放在同一 ToR 下，尽量放在同一 ToR;

4. DP 层不能都放在同一 ToR 下，但也不能过于分散，而是采用双层 AllReduce操作。（怎样放 DP 并行单元？）首先要判断处理业务时带宽是瓶颈还是聚合器资源是瓶颈。如果用满聚合器还用不满带宽，说明是聚合器瓶颈，反之则为带宽瓶颈。1. 如果是聚合器瓶颈，ToR 组数目开根号取整数上界 N ；如果是带宽瓶颈，且光带宽：ToR带宽=1/K，DP并行组数目为 D，那就取 $\sqrt{\dfrac{D}{K}}$ 的整数上界，就是二层聚合的连接数目，然后逐个放置 DP 并行单元，将其放置在当前剩余 server pool 资源池最宽裕的 ToR ，如果最宽裕的 ToR 上已经有 N 个 DP 并行单元了，那么就不再在其下放置 DP 并行单元了，直到用到 N 个 ToR ，然后再在 N 个 ToR 内找最宽裕的 ToR ，放完所有 DP 并行单元；2. 如果取下界发现大于一个 oxc 可以提供的最大连接数目，那么将 DP 组放置的 ToR 数目 N 固定为最大连接数目，找到 N 个剩余 server pool 资源最大的 ToR，然后按从大到小顺序逐个放 DP 单元，如果放完一轮没有把所有 DP 单元放完，那就再放一轮，直到所有 DP 组都被放完；3. 如果发现第一种情况 server pool 已经没有资源支持放置整个 DP 并行单元了，那么再多使用一个剩余资源最高的 ToR ，然后重新按规则从大到小放置，当一个 ToR 上的 DP 单元等于 N+1，就不再放了，以此类推；4. 如果发现 ToR 已经无法完整放下一个并行组了，也不能再加 ToR 了，那说明 moe 层会分开。逐个放置 DP 组内的 moe 层，先遍历已经使用的 ToR ，从剩余资源多的放起，剩余的 moe 层再找下一个剩余资源最多的 ToR。如果发现已使用 ToR 不够放，再找其他 ToR 放置 moe 层，直到全部 moe 层被放完。如果真的放不完所有 moe 层了，说明已经不可能把业务放进网络。

###### 备选方案2:

1. TP 组使用在服务器内部的 GPU 间进行；

2. 若干 worker 构成 EP 组，并作为一个 EP 层，worker 间采用 AllToAll 操作，组内 worker 间通信量不均且不稳定，放在同一个 ToR 下；

3. 每个 EP 层间使用 Pipeline 并行，可以放在不同的 ToR 下；

4. 不同 DP 单元的同一 moe 层可以放在同一 ToR 下。在训练时会做专家并行通信和 moe 层间通信，然后在每个 ToR 做一次 AllReduce 以同步参数，此时无需使用二级聚合。具体的放置方案为：找到业务处于相同 moe 层的所有不同 DP 单元，为其分组。按照 ToR 待传输数据量从小到大顺序逐个放置这些组，如果 server pool 已经被放满，那就放到下一个 ToR 上。如果已经放不下了，那么就要拆 DP 组，先在已放置该业务的 ToR 中按从小到大顺序遍历待传输数据量，尽力放置同一 moe 层的 DP 单元。如果还是放不下，哪就再找一个待传输数据量最多的 ToR 继续放置，直到放完。如果真的放不下了，说明这个网络已经完全无法为该业务服务。

###### 备选方案3：（备选1改进+新的 inc 拓扑）

1. 判断交换机何时会出现带宽瓶颈还是聚合器瓶颈，判断方法为：假设这个业务用满了这个 ToR 所有 OXC 带宽和与 server pool 之间的带宽，观察聚合器资源是否够用，；

2. TP 组使用在服务器内部的 GPU 间进行；

3. 若干 worker 组成 EP 组，作为一个 moe 层，worker 间采用 AllToAll 操作，放在同一个 ToR 下；

4. 每个 EP 层间使用 Pipeline 并行，通信量稳定且流量不高，可以放在同一 ToR 下，也可以不放在同一 ToR 下，尽量放在同一 ToR;

5. 首先确定最优情况下的 ToR 使用数目 N。如果是聚合器资源瓶颈，那么说明我们的部署理想情况下只会用满聚合器资源，此时单个 ToR 下的 DP 单元数目越少，则可以调用的聚合器资源越多，即 $N = 1$；如果是带宽资源瓶颈，若 ToR 上的 OXC 带宽：server pool 带宽 = 1:K，那么说明最好的情况下，一个 ToR 下最多可以部署 $\lfloor K \rfloor$ 个 ToR，即 $N = \lceil K \rceil$。a. 开始部署时，网络中的空余 ToR 数目相对较多，先占满所有 ToR ，以最大程度利用带宽资源。在部署业务时，逐个 DP 单元放进网络，先判断是否有 ToR 没有部署任何业务，如果还有，则找到剩余 server pool 资源剩余最多的 ToR ，判断该 ToR 上这个业务的 DP 单元数目是否已经到达了 $N$。如果到达了，那么就再找次多的 ToR，以此类推，反之则直接将此 DP 单元部署到该 ToR。如果发现

##### 拓扑和带宽调度方案：

1. 预测一个重构时间内的流量，计算初始拓扑，初始化连接：

2. 进行第一个拓扑重构时间内的调度：在业务通信需求开始时确定是否使用在网聚合，使用怎样的拓扑连接和路由策略，然后再计算所有业务此时的带宽分配，按照此路由方案和带宽分配处理业务，直到有通信需求开始或结束；

3. 预测下一个重构时间内的流量，找到拓扑中可以删除的（不可删除的：现有业务 ring 的预留连接，moe 层间连接）待传输和预约传输流量少的连接，如果断开的连接上有流量，让这个流量相关的任务先完成，然后断开并重构这一部分连接，将这些流量相关的后续业务强制改为使用 ring 方案，以适配下一个重构时间的流量；

4. 在完成第一个拓扑重构时间内开始的所有业务后，此时完成了第一次拓扑重构，检查是否有业务完成或出现，预测流量，找出下一个拓扑重构时间内可以删除的连接，强制改变业务聚合方案，删除连接，开始重构；

5. 循环按上述过程。

##### 初始拓扑求解：（找不到可用拓扑怎么办？）

1. 为所有在第一个重构时间的 ring 拓扑预留一根连接，如果存在 moe 层间跨机架互联，也要为其分配一条连接，由于 moe 层间通信在训练过程中且流量相较 DP 较小，将其看作训练过程的一部分，但是其没有替代连接，因此存在通信需求必须保留；

2. 找出所有在第一个拓扑重构时间内开始通信的业务，并分别计算只传该业务在网聚合下的时间$t_{inc}$和 ring-AllReduce $t_{ring}$的时间，为每个业务计算$\Delta = （t_{ring}-t_{inc}）*n$，其中n为业务在这段时间开始通信的次数；

3. 按$\Delta$降序排列，逐个处理业务，处理方式为：使用 inc 架构，计算二级聚合 ToR 需要多少聚合器资源，如果这个业务使用的一级聚合 ToR 上的剩余聚合器资源足够二级聚合，那么就选择聚合器资源宽裕的 ToR 作为二级聚合 ToR；如果不够二级聚合，那么就从所有 ToR 中选择资源宽裕者作为二级聚合 ToR ，根据流量比例分配连接，用满网络瓶颈。且并不是聚合器资源用满了就不能再分配了，毕竟并不是所有业务任何时候都需要做聚合。如果两个业务公用一个源/目的 ToR，则按照业务数据量重新按比例分配连接，用满网络瓶颈。如果发现业务已经没有多余的端口建立新连接了，标记之，说明这个业务无法支持在网聚合；

4. 如果存在多余的端口，增加相应 ring 拓扑的连接；

5. 遍历完所有业务，输出初始拓扑；

##### 业务开始传输的选择和调度方案：

1. 检查业务使用在网聚合需要的资源（带宽、聚合器）是否存在空闲（可能有多个聚合器和路由方案可以使用），使用 ring 的业务中途可以改变带宽，使用在网聚合的业务中途不改变带宽和聚合器数目分配，因为太麻烦。。。。

2. 如果不存在空闲，预测等待时间，即等待当前业务和所有预约业务完成聚合的时间$t_{wait}$，除此之外，还要看这个时间后，业务的瓶颈处什么时候有业务减少，完成业务后能腾出多少资源，记录；

3. 分别计算 ring 结构预测时间，ring 结构的带宽占用与业务数据量成比例，和使用在网聚合的时间，和等待后在网聚合的时间，找最小值，如果是 ring 最小，那就直接用 ring，反之，预约该聚合器的资源和带宽的资源的占用时段，从而确定二阶聚合的方案；

4. 根据二阶聚合的方案，如果二阶聚合采用 ring 结构，一阶聚合直接看是否有足够的资源即可，看瓶颈在什么时间后有业务减少，能腾出多少资源，分别计算在网聚合和 ring 的时间+等待时长，找到加上二阶聚合时间最低的方案；

5. 如果业务无法支持在网聚合，或是 ring 时间最小，那么按照传输总数据量大小比例分配带宽；

##### 冲突

1. 任务位于同一 ToR 下，无论是否使用在网聚合，带宽资源会出现瓶颈，需要多个 ToR 的带宽资源以缓解该问题；

2. 位于不同 ToR 下，OXC 端口资源会产生危险，且 INC 资源使用也会增加。最极端的情况是，所有 DP 并行单元位于不同的 pod 下，此时 oxc 资源使用和在有无 INC 都一样，两种方案此时差别不明显，不能很好的凸显 INC 的优势。如果使用在网聚合，瓶颈位于总通信量最大的交换机处，可以根据瓶颈处的带宽资源确定各 ToR 最大并行组数。或者说，各交换机聚合连接数需要尽可能匹配，避免出现带宽和聚合器瓶颈。

#### 不同并行方案的特点

1. EP 使用 AllToAll 通信，其通信并不均匀，也不可预测，而由于 EP 每次迭代计算时间都不一定长于重构延时，而固定的光拓扑又无法适应变化的流量，因此 EP 并行组应当尽量位于同一 ToR 下，以获得更加灵活的带宽分配；

2. DP 使用 AllReduce 通信。此时 DP 并行组会出现上面所说的冲突，需要妥善安置 DP 并行组的位置；

3. PP 流量少，连接资源消耗少，用的也少。。。；

4. TP 被放在服务器内部，使用 NVLink 互联解决其 AllReduce 流量大，通信繁琐的问题。

#### 部署方案：

##### Input

1. 业务队列；

2. 各业务参数：各并行度，考虑三种并行（EP/DP/PP），默认服务器内部使用 TP 不参与考虑，模型中最小的运算单元为 server，内部搭载 8 张用 NVLink 和 PCLe 互联的 GPU。采用三元组（e, d, p）表示并行度；业务使用 GPU 总数；业务训练总数据量和业务模型参数总量（据此计算单 GPU 训练数据量和模型大小，以估计单次训练时间与流量大小）；

3. 网络参数：pod 数目，单 pod server pool 大小，超额订阅比（1:1）

##### Output:

1. 业务部署方案

要求：减少后续调度复杂度。