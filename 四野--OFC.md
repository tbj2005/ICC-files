# 面向混合并行场景下AI流量的光互连数据中心网络调度

## 背景：

在大模型训练过程中，由于数据集合参数规模越来越大，单卡早已无法支持当前业务的训练，因此需要使用并行方案将单卡的训练压力稀释到多卡。AI 大模型训练任务需要多卡多节点分布式并行完成，集群规模扩展到万卡及以上数量级。当前 GPU 算力快速增长，网络通信正在拖大模型训练的后腿。

为了加速网络通信，可以引入光互连数据中心，利用其吞吐量高的优势，为通信提供更高的带宽，以缩短通信时间。然而，当前光互连 DCN 中，POD间的光连接拓扑重构延时高与大模型高效的传输需求并不符合，因此当前的大模型训练没有大规模采用光互连数据中心。在当前，计算和通信具有依赖性，在计算的时候链路往往是闲置的，计算资源和带宽资源总会有一个偷懒，不完全符合当优质帕鲁的要求。但是，由于AI流量具有时序性，可预测性强，可以通过设计面向AI流量的调度算法，将重构延时隐藏在计算过程中，扬长避短，提高训练过程中的网络通信能力。

混合并行是当前大模型训练的热门方案，具体是对同一训练模型采用多种并行方案，即复用分布式数据并行和模型并行，融合两种并行策略的优势以优化训练过程，降低单卡负担。其中，模型并行有多种方案，如流水线并行、张量并行等。

分布式数据并行将数据集均分成数据并行度大小数目的子集，从一轮迭代开始，在每个节点使用不同的子集训练相同大小的模型，然后每个节点再将得到的参数进行聚合和下发，然后开始下一轮的迭代，可以大幅降低节点训练数据量大小。节点间的通信可以使用参数服务器完成，但是参数服务器和其他 NPU 工作时间是错开的，没有高效利用 NPU 的资源，且在通信过程中参数服务器带宽压力过大也会导致瓶颈。在之前的工作中引入了在网计算这一项技术，缓解了参数服务器带宽压力过大的问题，但是网络中可能并不能满足所有参数服务器全部被取缔，因此仍会存在参数服务器，并不能完全解决上面的问题。而另一种通信算法 Ring-Allreduce可以解决上面的问题，通过回环同步不同 NPU 之间的梯度。

流水线并行是将训练过程分成若干个层，每一层可以分开训练上一层的输出可以作为下一层的输入，这样可以大幅降低节点内模型大小，防止模型爆掉节点的内存。然而，这样会导致 NPU 利用率极低，因此微软提出了 Gpipe 方案，即将数据集也分成若干子集，每个子集独立通过流水线完成训练过程，此时网络中 NPU 利用率大幅提高，且随着子集粒度的降低，网络中 NPU 利用率也会随之降低。

![朴素流水线并行和Gpipe](https://img-blog.csdnimg.cn/img_convert/15c54d10e976d35c96e57965145300e8.png)

张量并行是指将训练过程中常用的矩阵乘法进行行并行或是列并行，分别在不同节点计算后通过加和或是拼接的并行方案。这类并行方案可以切分很大的模型，但是通讯量大，通信一般只在节点内部做多卡通信，不会做跨节点通信。

总结：

- **数据并行**：将数据集分割成多个子集，在多个设备上分别处理这些子集。这种方式能显著提高数据处理速度，但需要确保子集之间处理结果一致。
- **流水线并行**：将模型按层分割成若干块，每块都交给一个设备进行处理。这种方式能提高设备的利用率，但需要确保流水线中各阶段正确传递中间结果。
- **张量并行**：将模型的不同部分分配给不同的设备进行处理。这种方式能充分利用各种设备的计算能力，但需要注意设备之间的通信开销。

三种并行方案的通信量为：张量并行 > 数据并行 > 流水线并行

![image-20240619210947461](C:\Users\tbj20\AppData\Roaming\Typora\typora-user-images\image-20240619210947461.png)

上图是一个三维混合并行方案。数据集首先被分成两个子集，分别使用上下两层 NPU 训练；在每层 NPU 中使用流水线并行分别进行训练，这里模型被分成 4 层，每层使用一列 NPU 处理；在每一层流水线中，一列 4 个 NPU 使用张量并行训练这一层模型。可以看出，混合模型融合了不同并行方案的优势，每个 NPU 需要训练的数据集大小和模型大小被大幅降低了。在训练大模型时使用混合并行方案，不同 NPU 间的通信情况不同，这也对合理的网络调度提出了要求。

## 问题提出：

在混合并行场景下，使用光网络加速AI大模型训练任务，利用AI流量通信需求具有时序性，可预测性强的特点，规划 NPU 使用、流量调度和光互连拓扑的重构。

## 优化思路：

具体来说，若数据并行度为 N，训练集大小为 D，光网络中 Pod 数目为 P。将这些 Pod分成 N 份作数据并行，每一份训练 mini-batch 数据集大小为 $\dfrac{D}{N}$，并训练完整的模型参数。在每一组数据并行的 Pod 中将 NPU 再分成 M 组以进行层数为 M 的流水线并行。将 mini-batch 再细分成 micro-batch，使用 Gpipe 方案进行以提高 NPU 利用率。流水线的每一层都由一个 server 完成，一个 server 内有 T 张 NPU，可以进行并行度为 T 的张量并行。此时为了训练这个模型，我们使用了 $N*M*T$ 张 NPU。

训练过程：

1. 将数据集分成 N 个 mini-batch，分给不同的数据并行单元；
2. 将每个 mini-batch 再分成 M 个 micro-batch，并进行数据流水线并行；
3. 在数据进入每层流水线后，server 内的 NPU 间使用张量并行。而张量并行的通信和计算都是在 server 内进行，server 内 NPU 间有着高容量的卡间带宽，可以满足张量并行的高通信量要求，具体张量并行的过程与调度无关；
4. 每次张量并行求得每一层流水线的结果后，需要在各层流水线间进行通信以进行前后向传输，在完成反向传播后，梯度汇总并进行参数更新，这个传输过程可能是跨 Pod 的，因为一个数据并行单元可能包含多个 Pod，因此需要使用光交换资源和 Pod 内带宽资源。
5. 在完成后向传输后，完成一次流水线操作，获得了 mini-batch 训练更新的参数，还需要通过 ring-Allreduce 更新数据并行单元的参数，此时也需要使用光交换资源

可以看到，整个过程使用了两次光网络资源，需要进行拓扑重配以最大化提供的带宽。

可以发现，在流水线并行完成反向传播后并不是马上去做数据并行的的通信，而是要等待梯度汇总和参数更新，在这段时间里，网络是闲着的，就可以把这段时间用来将拓扑从流水线转成数据并行；在模型训练过程中，第一个 micro-batch 的训练时，网络也是空着的，可以将拓扑再从数据并行转到流水线，以进行后续的流水线操作。

此时，训练过程中网络空闲的时间就被用于拓扑重构，隐藏了拓扑重构的时间，最大化地利用了光网络拓扑变化灵活，吞吐量大的优势。

对于网络而言，我们希望的是在未知混合并行方式和并行度，未知训练数据量和各卡训练通信时间，且无需改变现有训练方案的情况下，同时，网络也无法截取流量中的包，只通过网络中可获得的数据，来预测网络中的流量和连接拓扑。

由于 AI 流量的时序性，每隔一段时间，流量需求便会循环一次，直到完成训练，可以依据这一特点，来预测同步训练下每一次所有卡完成训练后的流量需求，并在训练过程中改变光连接，从而使得在 NPU 训练过程中，光网络并不是赋闲的，而是在利用这段时间改变拓扑重构，在利用光网络高吞吐量和灵活性的同时，规避重构时间长的风险。问题不在这个基本思路里，这种重叠时间的思路在很多地方已经用到了，不具有新颖性。

主要问题在于预测。因为网络是不知道整个模型什么时候完成了一次混合迭代，因此不好预测混合并行的周期数，而且至少网络在第一个迭代是完全不知道循环是怎样的，因此会产极高的延迟。因此我们需要一个合理的方案快速获得模型一次迭代的拓扑队列，且由于大模型的规模，模型收敛需要的迭代次数极高，因此后续完全匹配的拓扑队列对模型训练的增益将完全覆盖前几次迭代的损失。

但是，网络的效率仍然没有被完全发掘，因为我们只是在训练过程中使得网络没有停止工作，但是通信过程中，NPU 仍然在摸鱼。。。要解决这个问题，应该在训练方面下手，与光通信的调度无关。

说回算法，如前所述，我们要预测的其实是循环的一个周期，即最小周期预测。方案如下：

1. 初始化：拓扑队列为空，循环次数为0；
2. 在训练过程中执行如下操作：

​        a. 若循环次数为0，拓扑队列为空，说明这是第一次循环的第一次通信，在训练过程置空所有光连接拓扑，在获得通信需求后计算拓扑连接方案，并将该拓扑放入拓扑队列，并将循环次数置0；

​        b. 若循环次数为0，拓扑队列不为空，说明这是第一次循环的非首次通信，不修改上次的拓扑，在获得通信需求后计算拓扑连接方案，判断连接方案是否与队列首个元素相同。如果相同，判定为出现循环，标记循环次数为1。反之，如果不相同，判定为在循环内，将该拓扑放进拓扑队列，并将循环次数置0；

​        c. 若循环次数大于0，说明已经获得了拓扑队列，此时取拓扑队列首个元素，按此拓扑在训练过程修改光连接。获得通信需求，计算拓扑连接方案，并与取得的方案做匹配。若相同，则直接开始进行光通信，若遍历完一次拓扑队列，将循环次数加1，并进入下一次循环；若不相同，说明此时获得的周期并不是最小预测周期，而是这个周期中的一部分，此时，需要重构拓扑队列，并将循环次数置0。

​        重构队列的具体方案为：将原拓扑队列复制循环次数次，再加上此次循环已经遍历的部分，就是新的拓扑队列。

​        d. 训练收敛时，结束算法

LLM 的流量特征：

LLM 的训练会在每台主机上产生少量周期性爆发流量，如400 Gbps 网络容量会被瞬间满足，持续时间几秒到几十秒不等，梯度同步导致了此类流量的产生。这会导致等价多路径哈希极化（多层哈希后流量不均），造成不确定流量分布的问题。且训练产生的通信连接需求往往只有数十到数百条，远远低于云计算。

LLM 的训练会要求GPU同步迭代，此时会对单点故障更加敏感。

GPU 规模：每个主机 8 个 GPU，每个数据中心 1875 个主机，共 15000 个 GPU，网卡吞吐量约 400 Gbps

传统数据中心和 LLM 匹配度不高。这是因为传统数据中心使用 ECMP 方式作为负载平衡方案。ECMP 假定当网络中存在大量流量时，哈希算法可以有效地将所有流量平均分配到所有等效路径上。这一假设在云计算流量模式下成立，因为云计算流量模式一般会产生数百万流量，不易出现极化。但是 LLM 流量数目低，数据量大，即大象流，容易产生哈希极化，严重影响效率。甚至，经典三层 clos 架构（ToR、Aggregation、Core）会使用三次哈希，因为每个流量哈希的五元组保持相同，这种级联哈希的效果会导致更严重的哈希极化。

当前的只是将预测和重叠时间两个比较老套的方案结合在了一个新的场景而已。

算法可已知模型并行方案和并行度，数据量等信息，可加入动态场景的调度，此时循环会发生变化，可以尝试为多个LLM业务做调度。其次，还可以联合调度混合并行方案和集合通信方案，并与INC结合。比如一个数据中心可以动态地为多个LLM提供服务，此外，联合调度混合并行方案和集合通信方案和INC使用方案。

## 拓扑选择：直接连接 or 间接连接？

直接通信：速度快，延迟低，扩展性差

间接通信：速度相对慢，延迟高，扩展性和灵活性高

混合通信：服务器内搭载多张 GPU，GPU 间采用 NVL、PCLe、QPI 高速互联，由于 RDMA 等技术的使用，服务器内 GPU 间通信带宽高，延时低。但是服务器内 GPU 的通信复杂度阻碍了服务器内 GPU 数目的增长，此时在服务器间采用间接连接以提高网络的可扩展性，服务器间连接可以采用 clos 结构组成三层 Fat-tree （？）。服务器间的通信引入光网络，利用其带宽高，可以适应倾斜流量的特性，来加速训练过程中倾斜的集合通信流量。

## 混合并行方案的选择：

DP+PP+TP

模型并行是三种并行方式开销最大的，因此将其放在服务器中，使用服务器内的高速互联来处理。在模型并行中会产生 AllReduce 流量，这部分流量会通过 NVL 快速传输，无需调度，模型并行度由服务器内 GPU 数目等因素决定。

数据并行通信开销高于流水线并行，尽量使用光网络，发挥其倾斜性和高带宽的优势，因此第一层并行采用数据并行，具体的集合通信方案的选择按下不表。

流水线并行通信开销最低，只在有必要时使用光交换。

若已知训练数据量、参数量和并行度，可以估算各处训练时间和集合通信时间，可以以此作为调度的前置信息。

## 集合通信算法的选取：

张量并行要进行数据同步，在服务器内采用 AllReduce 操作；流水线并行只有 send-recv 流量，传输的是上一层流水线的输出数据，无需集合通信算法；数据并行传输的是每个数据并行组的参数，其中的集合通信算法待选取，最经典的两种算法如下：

parameter server：参数服务器存在带宽瓶颈，在参数服务器和其他训练节点工作时间不重叠。但是可以通过部署 INC 来解决这一问题，此时网络设备就可以作为参数服务器，不会出现上面的情况。

ring-AllReduce：同样可以解决传统 parameter server 架构的问题，但是由于没有聚合操作，无法使用 INC，且随着 ring 的增大，集合通信效率会大大降低。

上面所说的数据并行->流水线并行->张量并行方案是针对数据量而言的，但是当考虑了集合通信算法后，混合并行方案也许会有不同。

若采用 parameter server 算法，数据并行组需要尽量处于同一个 pod 下，从而避免跨机架聚合，且能大幅降低 oxc 中的流量，因此混合并行方案可以为：流水线并行->数据并行->张量并行。（不一定就要用这种并行方式，因为此时 ToR 上的流量就是整个模型的数据量，网络瓶颈就在 ToR 上行带宽，会拉低通信效率，所以这部分还需要权衡）；

若采用 ring-AllReduce 算法，此时因为数据并行通信数据量更大，混合并行量可以为：数据并行->流水线并行->张量并行

## 多 LLM 业务在 server 使用、集合通信方案和集合通信上的共同调度：

数据中心网络中可能存在多个 LLM 业务，其出现也可能是动态的。怎样选取这些业务的并行方案和集合通信方案，怎样调度这些业务的集合通信流量？

以通信方案为例，在 INC 资源足够时，采用 parameter server 架构显然更好，但是如果 INC 资源被其他业务使用了，采用 AllReduce 可能就是更好的方案。

集合通信可以通过之前估算的训练和通信时间调度，通过预测流量周期以在训练过程完成光网络重构。

### 集合通信算法：

#### Ps-worker:可用于数据并行

确定拓扑方便（只需确定数据并行单元部署位置即可），使用限制多（单 pod、ToR 上的 INC 资源），完全无需使用 OXC。

#### AllReduce 类：可用于数据并行

难以确定拓扑，不仅需要确定每个数据并行单元的位置，还需要确定这些数据并行单元怎样互联成环/树。

##### 环算法：ring-AllReduce

单节点接受发送数据量达到最小，输入输出带宽用满，但是如果不只考虑带宽，再考虑每次传输的延迟

使用线性通信开销模型,$T = \alpha+S/B$

$T = （N - 1）(2\alpha + 2\dfrac{S}{NB}+\dfrac{SC}{N})$

其中， $\alpha$为设备间的通信固有延时，S为数据块大小，N 为节点数目，B为节点带宽，C为每字节计算耗时。

当 S 极大时，固有延时 $\alpha$ 可以忽略，此时随着N 的增大，延时 T 与 N 无关，为$T = 2\dfrac{S}{B}+SC$。

但是当 S 特别小，或者 N 特别大时，后两项就被忽略了，此时 $T = 2(N-1)\alpha$，此时延时由固有延时决定，随着 N 的增大持续增加。这就是环算法的缺陷，正因如此，树算法才被开发出来。

##### 树算法：tree-AllReduce

树算法实际在英伟达 NCCL 投入使用的方案是双二叉树法，使用两个对称二叉树解决传统单二叉树算法节点输入输出带宽量不同的问题。

根据论文，其延迟为$T \leq 4\alpha\log N+\dfrac{2S}{B}+2\sqrt{8\log N\alpha\dfrac{S}{B}}+(\sqrt{2\log N\alpha S}+S)C$

可以看出树算法对 N 极大的情况表现比环算法好得多。

但是树算法在同样的节点数目情况下，需要建立的连接数比环算法多得多，拓扑的复杂度更高。

#### 分层 ring-AllReduce

即将环分为两层，可以缓解 N 极大带来的延迟提高。连接数多于普通环算法，但是总时间在 N 较大时更低。但是如果 N 非常大，树算法的表现更好。

#### AlltoAll: 只用于 MoE 并行

1. 为已知混合并行需求的业务提供服务；

2. 假设分别使用不同的集合通信算法，分别寻找合理的 GPU 部署方案和拓扑，选择最优的部署方案和通信算法

部署方案的优异程度怎样确定？

1. MLU

2. 集合通信是否需要重构 OXC，重构次数多少？

3. 在多业务情况下，不同时间集合通信复杂度不同，可能在某时刻只有一个业务需要通信，可能某时刻有多个业务同时有集合通信需求，如何联合考虑所有情况下通信时间的优劣？

背景：传统的数据中心网络在训练大模型时，会由于哈希极化现象，导致网络中的流量不均衡，从而造成网络瓶颈。而光互连由于可以通过灵活变更拓扑，使得网络可以适应倾斜流量，在理论上，表现会远远优于传统的数据中心网络。但是由于技术原因， oxc 的端口数目在大规模场景下难以支撑其实现全部 pod 间的互联，为了缓解这种情况，我们引入了 INC 技术来降低网络中的流量大小，使得有限的端口数目能够应对更多的流量场景，同时还可以通过联合优化并行策略和通信算法，进一步缓解这一问题。

1. 并行策略级：混合并行的策略已经有研究了，在 SmartMoE、Alpa等文章中已经讨论过多种混合并行方案怎样选择，和专家并行时 expert 如何部署的问题。在这一部分可以尝试再将 INC 引入考虑场景，进一步优化在有 INC 场景下的选择；

2. 通信算法级： PP 流量比较简单，无需特定的通信算法，TP 常常被放在服务器内部使用 NVLink 高效互联，不会占用网络的带宽，因此不在我们考虑范围内。问题主要集中在 EP 和 DP 两种并行方案上，其中 EP 每次迭代都会在 expert 间出现两次不均匀的 AllToAll 通信，这部分通信在 DeepspeedMoE 中已经做了优化，目前针对 EP 中的集合通信优化除此之外几乎没有其他的方案；DP 需要做 AllReduce ，存在多种已成熟的方案可供选择，当下主流的算法为 ring-AllReduce，但在英伟达的集合通信库中，使用的是 tree-AllReduce，在特定的场景中，也有不同的 AllReduce 算法被开发，如果将 INC 及其限制引入考虑场景，那么参数服务器也可以是一种待选择甚至更加优秀的方案，而网络中大概率不是只跑一个业务，对不同的业务来说，可能更适合采用不同的算法，算法的选取不仅和业务本身有关，还与并行策略中怎样部署相关的并行单元有关；

3. 流量调度级：确定 OXC 的连接方案，使得在网络拓扑不发生变动的情况下，逐个处理业务通信需求，从而使得网络的训练效率更高。如果网络拓扑不能满足每一个业务的通信需求，则在每一次迭代的过程中，网络都需要重构，而每次迭代网络重构延时数百毫秒，在动辄千万次迭代的大模型场景里是不可容忍的。这一部分如果要考虑的很细的话非常复杂，虽然网络中同一个业务的流量是周期性的，但是多个业务放到一起，情况就会变得非常复杂且不易调度，难以使用周期性质，因此这一部分目前看先按照最简单的方案，就是通信需求分开，不同的业务不能同时使用网络资源。

#### 

#### 问题描述：

在在线模式下，数据中心网络会随时出现分布式的混合并行训练业务，每个训练业务存在若干次迭代。已知每个业务各并行方式的并行度，怎样将业务的各并行单元分配给网络中各 pod 上的 GPU 才更合理？在确定业务部署方案后，每个业务在每次迭代的通信过程应当采用怎样的聚合策略和路由策略？每个业务分配的带宽量又是多少？

#### 问题输入：

1. 网络特征及各项参数：三层 clos 脊叶网络，ToR 与 server pool 间的带宽，oxc 连接数目;

2. 每个业务的并行方案和各层并行度，业务通信需求，业务参数量和流水线每层输出张量大小，业务每次迭代的训练用时；

3. 交换机聚合器资源；

#### 假设：

1. oxc的传输方案选择全双工，也就是说一条连接可以同时为两个方向的流量提供相同大小的带宽；

2. 由于张量并行一般在服务器内部进行，不会使用到外部带宽，因此混合并行中不考虑张量并行；

3. 不同并行方式下各并行单元间的通信模式：EP：AllToAll；PP：Send-Recv；DP：AllReduce，混合并行下三种流量都会存在；

4. EP 并行的 AllToAll 通信由于 Gate 概率问题不会是均匀的，为了简化考虑，认为该过程为单纯的数据块转置；

5. DP 并行的 AllReduce 通信可以使用在网聚合，但是并不是每次通信，在网聚合的资源都是空闲的，因此网络不仅要有在网聚合的能力，在没有在网聚合是也能通过相应的 Reduce 算法来解决。具体实现时在打包数据的时候把是否使用在网聚合放进 head 段用来识别聚合方案就可以了，配合路由方案选择的算法按理来说可以实现不同迭代使用不同聚合方案的操作;

6. 在网聚合可以分双层来做，此时有可能需要 OXC 连接，连接数目不大于 ring，通信量小于 ring，通信时间小于 ring。当一部分数据并行单元都在同一 ToR 下时，如果该 ToR 没有聚合器资源了，此时用在网聚合不一定比 ring 好；

7. 在网聚合资源建模：业务使用聚合器资源与输入 ToR 带宽成正比，聚合器资源用满时带宽不会用尽，即聚合器资源更加紧张。

#### 待解决的问题：

1. 部署问题：初始业务 GPU 部署问题，将 GPU 部署进网络后怎样部署各并行单元；

2. 流调度问题：为每个业务的通信计算两种 AllReduce 方案，分别是使用在网聚合和不使用在网聚合；在在线模式预测业务的通信需求，并为其确定通信方案，若使用在网聚合，确定使用的聚合器资源，若不使用，则确定网络拓扑和带宽分配，确定是否使用 oxc 重构。在通信过程中，不同迭代下的通信方案可以不同，用以适应不同场景。

3. 拓扑重构触发条件：有业务完成或有业务加入，导致网络中流量倾斜方式变化，此时需要重构以适配之。

#### 优化目标：

GPU 平均训练时长，由于未知迭代次数，无法计算平均完成时长，因此摒弃使用平均化时间的思路，转而提高 GPU 利用率。

#### 工作：

1. 梳理不同并行间的通信模式；

2. 整理算法；

3. 实验；

冲突：

1. 任务位于同一 ToR 下，无论是否使用在网聚合，带宽资源会出现瓶颈，需要多个 ToR 的带宽资源以缓解该问题；

2. 位于不同 ToR 下，OXC 端口资源会产生危险，且 INC 资源使用也会增加。最极端的情况是，所有 DP 并行单元位于不同的 pod 下，此时 oxc 资源使用和在有无 INC 都一样，两种方案此时差别不明显，不能很好的凸显 INC 的优势。如果使用在网聚合，瓶颈位于总通信量最大的交换机处，可以根据瓶颈处的带宽资源确定各 ToR 最大并行组数。或者说，各交换机聚合连接数需要尽可能相等。

3. EP 使用 AllToAll 通信，其通信并不均匀，也不可预测，而由于 EP 每次迭代计算时间都不一定长于重构延时，而固定的光拓扑又无法适应变化的流量，因此 EP 并行组应当尽量位于同一 ToR 下，以获得更加灵活的带宽分配；

4. DP 使用 AllReduce 通信。此时 DP 并行组会出现上面所说的冲突，需要妥善安置 DP 并行组的位置；

5. PP 流量少，连接资源消耗少，用的也少。。。；

6. TP 被放在服务器内部，使用 NVLink 互联解决其 AllReduce 流量大，通信繁琐的问题。

#### 部署方案：

##### Input

1. 业务队列；

2. 各业务参数：各并行度，考虑三种并行（EP/DP/PP），默认服务器内部使用 TP 不参与考虑，模型中最小的运算单元为 server，内部搭载 8 张用 NVLink 和 PCLe 互联的 GPU。采用三元组（e, d, p）表示并行度；业务使用 GPU 总数；业务训练总数据量和业务模型参数总量（据此计算单 GPU 训练数据量和模型大小，以估计单次训练时间与流量大小）；

3. 网络参数：pod 数目，单 pod server pool 大小，超额订阅比（1:1）

##### Output:

1. 业务部署方案

要求：减少后续调度复杂度。