% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{GPT4}
\BIBentryALTinterwordspacing
D.~Patel and G.~Wong, ``{GPT-4} architecture, infrastructure, training dataset,
  costs, vision, {MoE},'' Jul. 2023. [Online]. Available:
  \url{https://www.semianalysis.com/p/gpt-4-architecture-infrastructure.}
\BIBentrySTDinterwordspacing

\bibitem{surveyDML}
J.~Verbraeken, M.~Wolting, J.~Katzy, J.~Kloppenburg, T.~Verbelen, and J.~S.
  Rellermeyer, ``A survey on distributed machine learning,'' \emph{CSUR},
  vol.~53, pp. 1--33, Mar. 2020.

\bibitem{Meta}
\BIBentryALTinterwordspacing
L.~Kevin, G.~Adi, and O.~Mathew, ``Building {Meta}’s {GenAI}
  infrastructure,'' Mar. 2024. [Online]. Available:
  \url{https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/}
\BIBentrySTDinterwordspacing

\bibitem{HPN}
K.~Qian, Y.~Xi, J.~Cao, J.~Gao, Y.~Xu, Y.~Guan, B.~Fu, X.~Shi, F.~Zhu, R.~Miao,
  C.~Wang, P.~Wang, P.~Zhang, X.~Zeng, E.~Ruan, Z.~Yao, E.~Zhai, and D.~Cai,
  ``Alibaba {HPN}: A data center network for large language model training,''
  in \emph{Prof.of {SIGCOMM} 2024}, Aug. 2024, pp. 691--–706.

\bibitem{Parallelism}
D.~Narayanan, M.~Shoeybi, J.~Casper, P.~LeGresley, M.~Patwary, V.~Korthikanti,
  D.~Vainbrand, P.~Kashinkunti, J.~Bernauer, B.~Catanzaro, A.~Phanishayee, and
  M.~Zaharia, ``Efficient large-scale language model training on {GPU} clusters
  using {megatron-LM},'' in \emph{Prof.of {SC} 2021}, Nov. 2021, pp. 1--15.

\bibitem{SGD2010}
M.~Zinkevich, M.~Weimer, L.~Li, and A.~Smola, ``Parallelized stochastic
  gradient descent,'' in \emph{Proc. of {NeurIPS} 2010}, Dec. 2010, pp. 1--9.

\bibitem{PS}
M.~Li, D.~G. Andersen, J.~W. Park, A.~J. Smola, A.~Ahmed, V.~Josifovski,
  J.~Long, E.~J. Shekita, and B.-Y. Su, ``Scaling distributed machine learning
  with the parameter server,'' in \emph{Proc. of {OSDI} 2014}, Oct. 2014, pp.
  583--598.

\bibitem{INC2019}
\BIBentryALTinterwordspacing
N.~Zilberman, ``In-network computing,'' Apr. 2019. [Online]. Available:
  \url{https://www.sigarch.org/in-network-computing-draft/.}
\BIBentrySTDinterwordspacing

\bibitem{Rina}
Z.~Chen, X.~Liu, M.~Li, Y.~Hu, H.~Mei, H.~Xing, H.~Wang, W.~Shi, S.~Liu, and
  Y.~Xu, ``Rina: Enhancing ring-allreduce with in-network aggregation in
  distributed model training,'' in \emph{Prof. of {ICNP} 2024}, Jul. 2024.

\bibitem{Tree}
P.~Sanders, J.~Speck, and J.~L. Tr\"{a}ff, ``Full bandwidth broadcast,
  reduction and scan with only two trees,'' in \emph{Prof. of {PVM/MPI} 2007},
  Sep. 2007, p. 17–26.

\bibitem{MoE}
W.~Cai, J.~Jiang, F.~Wang, J.~Tang, S.~Kim, and J.~Huang, ``A survey on mixture
  of experts,'' \emph{ACM Comput. Surv}, Oct. 2024.

\bibitem{Zerwas2021}
J.~Zerwas, W.~Kellerer, and A.~Blenk, ``What you need to know about optical
  circuit reconfigurations in datacenter networks,'' in \emph{Proc. of {ITC
  2021}}, Aug./Sept. 2021, pp. 1--9.

\end{thebibliography}
