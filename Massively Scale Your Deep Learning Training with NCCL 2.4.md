[Massively Scale Your Deep Learning Training with NCCL 2.4 | NVIDIA Technical Blog](https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2-4/)

想象一下使用成千上万个 GPU 来训练你的神经网络。使用多个 GPU 进行神经网络训练在所有深度学习框架中已经相当普遍，提供了优化的多 GPU 和多机器训练能力。Allreduce 操作，用于在多个 GPU 上对梯度求和，通常使用环形结构 来实现以达到全带宽。环形结构的缺点是随着 GPU 数量线性增加，延迟也会线性增加，限制了超过数百个 GPU 的扩展能力（？）。于是，NCCL 2.4 应运而生。

许多大规模实验已经将传统的平坦环形结构替换为层次化的二维环算法，以获得相当不错的带宽同时降低延迟。

现在，NCCL 2.4 引入了双二叉树算法，提供全带宽和比二维环结构延迟更低的对数级延迟。

双二叉树在2009年于MPI提出，可以同时提供 broadcast 和 reduce 操作的全带宽，这两个操作又可以合并成 AllReduce 操作，即先 reduce 后 broadcast，且延时为对数级，使其在小中型的操作中的表现更好。

在 NCCL 中，我们使用一个易于实现的模式，即最大本地化建立了一个二叉树

双二叉树依靠这样一个事实建立：二叉树的大半节点为叶子节点，而小半节点不是叶子节点。因此，我们可以通过将叶子节点和节点互换来构建第二棵树，每棵二叉树都是如此。可能有一个成员在两棵树中都是叶子节点，但没有成员同时在两棵树中充当节点。

![](C:\Users\tbj20\AppData\Roaming\marktext\images\2024-07-29-12-10-45-image.png)

上图的两个二叉树互补，左图奇数成员作为叶子节点，而有图偶数成员作为叶子节点。重叠两棵树，除了根节点（0，31）只有一个父节点和一个子节点外，所有节点都有两个父节点和两个子节点。如果我们用两棵树中的每一棵都处理一半的数据，每个成员做多会接收和发送两次数据，每次数据量都为总数居的一半，在数据发送和接收方面和环算法一致。

大规模场景的表现：

规模：24576个GPU

树算法比环算法显然拥有更低的延迟；在维持全带宽方面，表现也优于环算法

虽然这种方法并非完美，但未来可能会有改进。即便如此，树结构仍然显示出明显的优势，即使带宽受限，也因其较小的初始延迟而具有优势。然而，当环形结构可以提供更大带宽时，NCCL会自动切换回环形结构。

相比于单二叉树，双二叉树的优势在于可以完全使用接收的发送带宽，单二叉树有一半节点只发送，不接收。但是数不好对数据分块，因为如果分块后，不同索引的数据块会同时进入根节点，这会出现混乱。
