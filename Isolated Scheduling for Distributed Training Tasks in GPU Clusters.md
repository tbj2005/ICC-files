### Isolated Scheduling for Distributed Training Tasks in GPU Clusters

1. 分析在 32 Nvidia V100 GPU 集群中网络争用怎样影响训练时间；

2. 提出 vClos 架构，通过联合优化网络拓扑、路由和通信模式以消除网络争用，增加网络资源利用；

为了避免在 DML 训练过程中网络成为瓶颈，许多训练架构把 AllReduce 通信与反向传播时的计算时间重叠。然而，网络还是会成为限制因素，理由如下：

1. NLP 等 DML 业务需要在训练的时候使用 AllToAll 通信，隐藏其通信开销比 AllReduce 更加复杂；

2. DML 完全使用网络中的带宽资源是困难的，仅仅增加带宽策略不能完全解决网络瓶颈；

3. 现代数据中心网络才有 ECMP 多路径负载平衡协议，会出现哈希极化现象，会导致不同业务的带宽争用。

争用来源：

1. 任务内争用：任务的通信阶段包含若干过程，每个过程都有不同的通信模式。可以为不同的过程设计不同的免争用规则，但是在交换机更新这些规则需要花费至少10微秒，慢且易出错；

2. 任务间争用：当一个网络连接被多个任务使用，这些任务可能会竞争网络带宽。可以为每个业务预留一个连接相邻子拓扑，但是可能导致二外的资源碎片化。例如即使有足够的 GPU 资源，因为不充足的网络资源，一个 DML 业务仍然不能被初始化。资源碎片化对 GPU 资源的利用不利，还会增加等待时长。

本文关注为每个业务分割 clos 子拓扑以避免争用。

源路由：在源节点就将部分或全部路径放入包头；

目的路由：在传输过程中自由选择能到达目的节点的路径。

vClos 方案可以解决脊叶网络中 DML 业务的挑战。针对单个DML任务而言，vClos是Leaf-Spine子拓扑的一个较小实例，并采用了源路由策略，可以证明在任何Leaf-wise排列的流量模式下都是无冲突的。通过适当分配GPU，可以塑造大多数通信模式以符合这种模式。

vClos 还使用了可选择的 OCS 层，以减少资源碎片化，同时保证了不同 DML 之间的网络隔离，且 OCS 的功耗可以忽略不计。当一个 DML 业务由于网络资源不足无法调度时，OCS-vClos 可以通过控制 OCS 层重新配置网络拓扑，使得任务可以部署。

哈希极化是数据中心的一个常见问题。

DML 业务的扩展因子：$\dfrac{T_n}{nT}$，其中，T 是但设备吞吐量，$T_n$是 n 个设备组成的系统的总吞吐量。理想情况下为1，在实际模型和环境中，通信瓶颈造成的损失最大。

网络冲突对不同模型的影响：

1. VGG 16 和 BERT 这类任务，网络冲突会造成明显的吞吐量下降，这可能是因为他们相关参数数目更大导致的；

2. 对数据并行模型来说，batch 数目越大，冲突影响越小，这可能因为 batch 数目大导致计算开销更容易被重叠；

3. DLRM 和 MoE 对冲突更敏感，这是因为它们采用 AlltoAll 通信，难以重叠；

4. 在较小带宽下运行的 DML 任务对网络争用更敏感，这可能有两个原因：首先，在较小带宽下，不可覆盖的通信开销会增加；其次，通信算法的同步开销会阻碍网络带宽的充分利用，这意味着网络争用的严重程度对 DML 训练的性能有非线性影响，因此避免哈希碰撞以减少流量争用的次数是有意义的。

对多租户集群来说，网络竞争对任务平均完成时间（JCT）的影响可能更为明显。

服务器内部带宽：Tbps-levels

网卡可提供带宽：100/200Gbps，为了防止 PCIE 争用，一个 GPU 只能使用一张网卡。

AllReduce 的流数据大小原则上在数百万字节到数千万字节，大部分的通信开销可以被反向计算重叠，然而通信开销在多路径竞争带宽时可能不好重叠。

模型并行的通信不能和计算时间重叠，为了最大化利用服务器内部带宽，一般会在服务器内处理。

流水线并行，只在相邻 GPU 之间存在收发流量，相比于数据并行，流水并行的通信量更少，但是其通信开销和计算开销仍然无法重叠。

专家并行在大模型训练中被广泛使用。MoE层存在于专家并行性中，由分布在不同工作节点上的多个专家组成。MoE层需要不同专家之间的 AlltoAll 通信。相比于数据并行，AlltoAll 通信开销仍然不能被重叠，流量大小大约为数千万字节，因此混合专家并行对网络争用十分敏感。

经典大模型训练可能会混合多种并行方案，我们分析了一个拥有6000亿参数的大型模型的训练开销，未能避免网络争用的情况下，其中全互联通信开销占25.8%，而全局归约通信开销占4.2%。

难以实现计划路由以消除争用的原因：

1. 如果修改路由表影响现有的路由表，可能会出现丢包，在大规模网络下可能会导致 PFC 风暴；

2. 出于隐私考虑，很难直接感知公共云环境中的用户流量特征。

因此我们希望可以不通过感知流量模式的方式，消除网络争用，主要关注了两种场景：

1. 单业务场景：可以通过在脊叶网络中设计一个无冲突路由来解决。具体来说，在叶交换机有一个调度者在一些业务到达时分发 ACL 流表来调度路由；在脊交换机上使用 BGP 协议或是静态路由直接产生路由表。使用这种方式，流量的争用就会被消除；

2. 多业务场景：调度者试图为脊叶网络中的每个业务划分子拓扑 vClos，以隔离业务间的流量。为了减少潜在的网络碎片化，还需要在叶交换机和脊交换机间引入 OCS 层，通过 OCS 重构减轻网络碎片化。

业务->业务队列->选择业务，告知业务需求->资源调度

选择业务方案：FIFO（先进先出），SF（小者先出）、EDF（deadline 早者先出）

初步消除网络竞争：

消除网络竞争的困难：

1. 交换机转发规则的更新无法在如此短的时间内完成；

2. 另一种消除争用的方案是建立一个严格无阻塞的脊叶网络，这个方案仍然在所有叶和所有脊中创造了一个均匀二分图。但是叶交换机的上行链路 $p_u$和下行链路 $p_d$数目需要满足 $p_u\geq2*p_d-1$。当新流量到达后，必须存在一个脊交换机，这样，从源叶子到脊交换机，和从脊交换机到目标叶子的连接就不是被占领的。这样，计算新流量的转发规则就是简单的，且在交换机上更新转发规则不会影响正在转发的流量。然而存在以下的问题：1. 增加新的转发规则会需要不可忽视的时间；2. 严格非阻塞脊叶结构比可再布置的非阻塞脊叶结构效率更低。例如，使用64port交换机，我们可以建立一个有着2048个GPU，96个交换机的可重布置的非阻塞网络，但是严格非阻塞结构只能建立 1344 个GPU，105个交换机的网络。

叶节点逐排列置换流量模式
